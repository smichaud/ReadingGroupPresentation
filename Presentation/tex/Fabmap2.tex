\section{FAB-MAP 2.0}

\subsection{Introduction}
\begin{frame}{FAB-MAP 2.0}
    \begin{itemize}
        \item What happened after version 1.0 ?
    \end{itemize}
    \begin{quotation}
        We describe a formulation which preserves almost all the key features of our earlier model, but allows for the exploitation of the sparsity of visual word data to achieve large reductions in computation and memory requirements.~\cite{fabmap2011}
    \end{quotation}
    \begin{itemize}
        \item General intuition of the work in 2.0
            %\begin{itemize}
            %\item Multiple changes for scalability
            %\end{itemize}
    \end{itemize}
    \note[item]{Read quotation}
    \note[item]{I will not go into details... General intuition/main points.}
\end{frame}

\subsection{Modifications}
\begin{frame}
    \frametitle{Probabilistic Model}
    \begin{itemize}
        \item Sparse approximation
        \item Inverted index
        \item Appearance likelihood changed in two ways
            \begin{enumerate}
                \item Restrictions on the probabilities in the location models
                \item Data association
            \end{enumerate}
        \item Implementation in O(\#vocab)
    \end{itemize}
    \note[item]{Sparse approx of the FAB-MAP model}
    \note[item]{Implementation using inverted index... What is it ?}
    \note[item]{If a document is considered as a list of word identifiers, then an inverted index maintains the inverse mapping from words to documents.}
    \note[item]{Appearance likelihood was changed in two ways to work with inverted index}
    \note[item]{Negative observations greatly outnumber positive ones, and are also generally less informative}
    \note[item]{Ignoring negatives yield bad results}
    \note[item]{*Single* probabilistic term for all observations of a place. Lead to some (negligible) information lost}
    \note[item]{Data association: sample-based instead of average appearance of a location (Location == set of samples.... Better suited for multi-modal)}
    \note[item]{They give the example of the open vs closed door}
    \note[item]{Complexity... because of CLT ~= O(1), worst case O(\#vocab)}
\end{frame}

\begin{frame}{Geometric Verification}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{./media/geo_verif.jpg}
    \end{center}
    \begin{itemize}
        \item Tested on the 100 most likely samples
        \item Essential at large scale
    \end{itemize}
    \note[item]{Brief description of geometric verification}
    \note[item]{Do it on the best 100 samples}
    \note[item]{Also talk about Motion Model, but this is not new}
    \note[item]{Likely to be at one of the topologically adjacent locations}
    \note[item]{Used as a prior for the probabilistic model}
\end{frame}

\begin{frame}{K-Means}
    \begin{itemize}
        \item Approximation of the k-means algorithm \citet{kmeans}
        \item Fixed-radius incremental pre-clustering
        \item Cluster merging heuristic
        \item This boosted performance of the system
    \end{itemize}
    \note[item]{To maintain system performance at scale, they had to make some changes}
    \note[item]{Large density variations in feature space}
    \note[item]{After each k-means iteration, if any two cluster centers are closer than a fixed threshold, one of the two cluster centers is re-initialized to a random location.}
\end{frame}

% [TODO]: Talk about Naives bayes vs CLT - 2015-09-26 01:36pm

\begin{frame}{K-Means Words}
    \includegraphics[width=0.5\textwidth]{./media/words_fabmap2a.png}
    \includegraphics[width=0.5\textwidth]{./media/words_fabmap2b.png}
    \note[item]{Result changes for fixed-radius initialization}
\end{frame}

\subsection{Results}
\begin{frame}{Results}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{itemize}
                \item Ground thruth: GPS
                \item Same place < 40m
                    \begin{itemize}
                        \item 89\% < 5m
                        \item 98\% < 10m
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.6\textwidth}
            \includegraphics[width=1.0\textwidth]{./media/map_fabmap2.png}
        \end{column}
    \end{columns}
    \note[item]{Validate the work on a 1000km data set}
    \note[item]{Really aimed at large scale (other with 66km but they have the biggest)}
\end{frame}

\begin{frame}{Geometric Verification}
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./media/results_geo_verif.png}
    \end{center}
\end{frame}

\begin{frame}{K-Means}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{./media/precision_recall_kmeans.png}
    \end{center}
    \note[item]{Not much to say except that the results are better}
    \note[item]{I just wonder why they show the curve when they want 100\% precision}
\end{frame}

\begin{frame}{Chow Liu and Naive Bayes}
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./media/results2.png}
    \end{center}
    \note[item]{Maybe table 3, indicate that CL vs NB for 100\% precision}
\end{frame}

\begin{frame}{Comparison TF-IDF}
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./media/results_vs_tfidf.png}
    \end{center}
    \note[item]{Compared with tf-idf ranking measurements}
\end{frame}

\begin{frame}{Processing Time}
    \begin{itemize}
        \item 14ms filter update + geometric verification
        \item 423ms SURF
        \item 60ms quantization
        \item 4400 times faster than 1.0
    \end{itemize}
\end{frame}
